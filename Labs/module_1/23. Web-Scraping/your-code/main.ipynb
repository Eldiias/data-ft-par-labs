{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended content.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit the urls below and take a look at their source code through Chrome DevTools. You'll need to identify the html tags, special class names, etc used in the html content you are expected to extract.\n",
    "\n",
    "**Resources**:\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are already imported for you. If you prefer to use additional libraries feel free to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "#using parser lxml to make html readable\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = soup.select('h1.h3.lh-condensed')\n",
    "names_select = [i.text.strip('\\n').strip() for i in soup.select('h1.h3.lh-condensed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nicknames = soup.select('p.f4.text-normal')\n",
    "nicknames_select = [i.text.strip('\\n\\n').strip() for i in nicknames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Marc Wouts': 'mwouts',\n",
       " 'CrazyMax': 'crazy-max',\n",
       " 'Jirka Borovec': 'Borda',\n",
       " 'Ross Wightman': 'rwightman',\n",
       " 'Armin Ronacher': 'mitsuhiko',\n",
       " 'Vladimir Agafonkin': 'mourner',\n",
       " 'Olivier Halligon': 'AliSoftware',\n",
       " '二货机器人': 'zombieJ',\n",
       " 'Francesco Biscani': 'bluescarni',\n",
       " 'Steve Smith': 'ardalis',\n",
       " 'David Rodríguez': 'deivid-rodriguez',\n",
       " 'Lovell Fuller': 'lovell',\n",
       " 'van Hauser': 'vanhauser-thc',\n",
       " 'Kévin Dunglas': 'dunglas',\n",
       " 'Nicolas De loof': 'ndeloof',\n",
       " 'Mislav Marohnić': 'mislav',\n",
       " 'Evan Wallace': 'evanw',\n",
       " 'Amaury': 'amaurym',\n",
       " 'Rico Suter': 'RicoSuter',\n",
       " 'Filippo Valsorda': 'FiloSottile',\n",
       " 'Alec Thomas': 'alecthomas',\n",
       " 'Igor Wojda': 'igorwojda',\n",
       " 'Gildas Garcia': 'djhi',\n",
       " 'Hans-Kristian Arntzen': 'HansKristian-Work',\n",
       " 'Nick Butcher': 'nickbutcher'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = dict(zip(names_select, nicknames_select))\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub.\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['willmcgugan /rich',\n",
       " 'edeng23 /binance-trade-bot',\n",
       " 'activeloopai /Hub',\n",
       " 'public-apis /public-apis',\n",
       " 'ansible /ansible',\n",
       " 'CorentinJ /Real-Time-Voice-Cloning',\n",
       " 'huggingface /datasets',\n",
       " 'TheAlgorithms /Python',\n",
       " 'PyCQA /pylint',\n",
       " 'OWASP /CheatSheetSeries',\n",
       " 'fishtown-analytics /dbt',\n",
       " 'deepset-ai /haystack',\n",
       " 'gvanrossum /patma',\n",
       " 'rwightman /pytorch-image-models',\n",
       " 'scipy /scipy',\n",
       " 'CryptoSignal /Crypto-Signal',\n",
       " 'nsidnev /fastapi-realworld-example-app',\n",
       " 'stypr /clubhouse-py',\n",
       " 'celery /celery',\n",
       " 'open-telemetry /opentelemetry-python',\n",
       " 'davidsandberg /facenet',\n",
       " 'd2l-ai /d2l-en',\n",
       " 'prompt-toolkit /python-prompt-toolkit',\n",
       " 'mementum /backtrader',\n",
       " 'mher /flower']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "trend_repo = soup.select('h1>a')\n",
    "trending_repo = [i.text.strip(' /\\n\\n').replace('/\\n\\n      ', '/') for i in trend_repo]\n",
    "trending_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Disney_Display_Case.JPG/170px-Disney_Display_Case.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/30px-Animation_disc.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/29px-P_vip.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/30px-Video-x-generic.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/30px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/30px-Blank_television_set.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/30px-Flag_of_the_United_States.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/22px-Commons-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/25px-Wikiquote-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/30px-Wikidata-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1',\n",
       " '/static/images/footer/wikimedia-button.png',\n",
       " '/static/images/footer/poweredby_mediawiki_88x31.png']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "selection = soup.select('img[src]')\n",
    "link_images = [i['src'] for i in selection]\n",
    "link_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#mw-head',\n",
       " '#searchInput',\n",
       " 'https://en.wiktionary.org/wiki/Python',\n",
       " 'https://en.wiktionary.org/wiki/python',\n",
       " '/wiki/Pythons',\n",
       " '/wiki/Python_(genus)',\n",
       " '#Computing',\n",
       " '#People',\n",
       " '#Roller_coasters',\n",
       " '#Vehicles',\n",
       " '#Weaponry',\n",
       " '#Other_uses',\n",
       " '#See_also',\n",
       " '/w/index.php?title=Python&action=edit&section=1',\n",
       " '/wiki/Python_(programming_language)',\n",
       " '/wiki/CMU_Common_Lisp',\n",
       " '/wiki/PERQ#PERQ_3',\n",
       " '/w/index.php?title=Python&action=edit&section=2',\n",
       " '/wiki/Python_of_Aenus',\n",
       " '/wiki/Python_(painter)',\n",
       " '/wiki/Python_of_Byzantium',\n",
       " '/wiki/Python_of_Catana',\n",
       " '/wiki/Python_Anghelo',\n",
       " '/w/index.php?title=Python&action=edit&section=3',\n",
       " '/wiki/Python_(Efteling)',\n",
       " '/wiki/Python_(Busch_Gardens_Tampa_Bay)',\n",
       " '/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)',\n",
       " '/w/index.php?title=Python&action=edit&section=4',\n",
       " '/wiki/Python_(automobile_maker)',\n",
       " '/wiki/Python_(Ford_prototype)',\n",
       " '/w/index.php?title=Python&action=edit&section=5',\n",
       " '/wiki/Python_(missile)',\n",
       " '/wiki/Python_(nuclear_primary)',\n",
       " '/wiki/Colt_Python',\n",
       " '/w/index.php?title=Python&action=edit&section=6',\n",
       " '/wiki/PYTHON',\n",
       " '/wiki/Python_(film)',\n",
       " '/wiki/Python_(mythology)',\n",
       " '/wiki/Monty_Python',\n",
       " '/wiki/Python_(Monty)_Pictures',\n",
       " '/w/index.php?title=Python&action=edit&section=7',\n",
       " '/wiki/Cython',\n",
       " '/wiki/Pyton',\n",
       " '/wiki/Pithon',\n",
       " '/wiki/File:Disambig_gray.svg',\n",
       " '/wiki/Help:Disambiguation',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&oldid=997582414',\n",
       " '/wiki/Help:Category',\n",
       " '/wiki/Category:Disambiguation_pages',\n",
       " '/wiki/Category:Human_name_disambiguation_pages',\n",
       " '/wiki/Category:Disambiguation_pages_with_given-name-holder_lists',\n",
       " '/wiki/Category:Disambiguation_pages_with_short_descriptions',\n",
       " '/wiki/Category:Short_description_is_different_from_Wikidata',\n",
       " '/wiki/Category:All_article_disambiguation_pages',\n",
       " '/wiki/Category:All_disambiguation_pages',\n",
       " '/wiki/Category:Animal_common_name_disambiguation_pages',\n",
       " '/wiki/Special:MyTalk',\n",
       " '/wiki/Special:MyContributions',\n",
       " '/w/index.php?title=Special:CreateAccount&returnto=Python',\n",
       " '/w/index.php?title=Special:UserLogin&returnto=Python',\n",
       " '/wiki/Python',\n",
       " '/wiki/Talk:Python',\n",
       " '/wiki/Python',\n",
       " '/w/index.php?title=Python&action=edit',\n",
       " '/w/index.php?title=Python&action=history',\n",
       " '/wiki/Main_Page',\n",
       " '/wiki/Main_Page',\n",
       " '/wiki/Wikipedia:Contents',\n",
       " '/wiki/Portal:Current_events',\n",
       " '/wiki/Special:Random',\n",
       " '/wiki/Wikipedia:About',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Contact_us',\n",
       " 'https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en',\n",
       " '/wiki/Help:Contents',\n",
       " '/wiki/Help:Introduction',\n",
       " '/wiki/Wikipedia:Community_portal',\n",
       " '/wiki/Special:RecentChanges',\n",
       " '/wiki/Wikipedia:File_Upload_Wizard',\n",
       " '/wiki/Special:WhatLinksHere/Python',\n",
       " '/wiki/Special:RecentChangesLinked/Python',\n",
       " '/wiki/Wikipedia:File_Upload_Wizard',\n",
       " '/wiki/Special:SpecialPages',\n",
       " '/w/index.php?title=Python&oldid=997582414',\n",
       " '/w/index.php?title=Python&action=info',\n",
       " '/w/index.php?title=Special:CiteThisPage&page=Python&id=997582414&wpFormIdentifier=titleform',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452',\n",
       " '/w/index.php?title=Special:DownloadAsPdf&page=Python&action=show-download-screen',\n",
       " '/w/index.php?title=Python&printable=yes',\n",
       " 'https://commons.wikimedia.org/wiki/Category:Python',\n",
       " 'https://af.wikipedia.org/wiki/Python',\n",
       " 'https://als.wikipedia.org/wiki/Python',\n",
       " 'https://ar.wikipedia.org/wiki/%D8%A8%D8%A7%D9%8A%D8%AB%D9%88%D9%86_(%D8%AA%D9%88%D8%B6%D9%8A%D8%AD)',\n",
       " 'https://az.wikipedia.org/wiki/Python',\n",
       " 'https://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%BE%E0%A6%87%E0%A6%A5%E0%A6%A8_(%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A4%E0%A6%BE_%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A6%B8%E0%A6%A8)',\n",
       " 'https://be.wikipedia.org/wiki/Python',\n",
       " 'https://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%BF%D0%BE%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)',\n",
       " 'https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)',\n",
       " 'https://da.wikipedia.org/wiki/Python',\n",
       " 'https://de.wikipedia.org/wiki/Python',\n",
       " 'https://eo.wikipedia.org/wiki/Pitono_(apartigilo)',\n",
       " 'https://eu.wikipedia.org/wiki/Python_(argipena)',\n",
       " 'https://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%8C%D8%AA%D9%88%D9%86',\n",
       " 'https://fr.wikipedia.org/wiki/Python',\n",
       " 'https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%84%A0',\n",
       " 'https://hr.wikipedia.org/wiki/Python_(razdvojba)',\n",
       " 'https://io.wikipedia.org/wiki/Pitono',\n",
       " 'https://id.wikipedia.org/wiki/Python',\n",
       " 'https://ia.wikipedia.org/wiki/Python_(disambiguation)',\n",
       " 'https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)',\n",
       " 'https://it.wikipedia.org/wiki/Python_(disambigua)',\n",
       " 'https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%AA%D7%95%D7%9F',\n",
       " 'https://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%98%E1%83%97%E1%83%9D%E1%83%9C%E1%83%98_(%E1%83%9B%E1%83%A0%E1%83%90%E1%83%95%E1%83%90%E1%83%9A%E1%83%9B%E1%83%9C%E1%83%98%E1%83%A8%E1%83%95%E1%83%9C%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%90%E1%83%9C%E1%83%98)',\n",
       " 'https://kg.wikipedia.org/wiki/Mboma_(nyoka)',\n",
       " 'https://la.wikipedia.org/wiki/Python_(discretiva)',\n",
       " 'https://lb.wikipedia.org/wiki/Python',\n",
       " 'https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)',\n",
       " 'https://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%BE%E0%A4%AF%E0%A4%A5%E0%A5%89%E0%A4%A8_(%E0%A4%86%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%B5%E0%A4%B2%E0%A5%80_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE)',\n",
       " 'https://nl.wikipedia.org/wiki/Python',\n",
       " 'https://ja.wikipedia.org/wiki/%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3',\n",
       " 'https://no.wikipedia.org/wiki/Pyton',\n",
       " 'https://pl.wikipedia.org/wiki/Pyton',\n",
       " 'https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)',\n",
       " 'https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)',\n",
       " 'https://sk.wikipedia.org/wiki/Python',\n",
       " 'https://sr.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%B2%D0%B8%D1%88%D0%B5%D0%B7%D0%BD%D0%B0%D1%87%D0%BD%D0%B0_%D0%BE%D0%B4%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%86%D0%B0)',\n",
       " 'https://sh.wikipedia.org/wiki/Python',\n",
       " 'https://fi.wikipedia.org/wiki/Python',\n",
       " 'https://sv.wikipedia.org/wiki/Pyton',\n",
       " 'https://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%9E%E0%B8%97%E0%B8%AD%E0%B8%99',\n",
       " 'https://tr.wikipedia.org/wiki/Python',\n",
       " 'https://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%84%D0%BE%D0%BD',\n",
       " 'https://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%A6%DB%8C%D8%AA%DA%BE%D9%88%D9%86',\n",
       " 'https://vi.wikipedia.org/wiki/Python',\n",
       " 'https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452#sitelinks-wikipedia',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License',\n",
       " '//creativecommons.org/licenses/by-sa/3.0/',\n",
       " '//foundation.wikimedia.org/wiki/Terms_of_Use',\n",
       " '//foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " '//www.wikimediafoundation.org/',\n",
       " 'https://foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " '/wiki/Wikipedia:About',\n",
       " '/wiki/Wikipedia:General_disclaimer',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Contact_us',\n",
       " '//en.m.wikipedia.org/w/index.php?title=Python&mobileaction=toggle_view_mobile',\n",
       " 'https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute',\n",
       " 'https://stats.wikimedia.org/#/en.wikipedia.org',\n",
       " 'https://foundation.wikimedia.org/wiki/Cookie_statement',\n",
       " 'https://wikimediafoundation.org/',\n",
       " 'https://www.mediawiki.org/']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "links = [i['href'] for i in soup.select('a[href]')]\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the number of titles that have changed in the United States Code since its last release point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "# soup.select('div.item_left')\n",
    "# print(soup.prettify())\n",
    "# [i['name'] for i in soup.select('a[name]').get_text()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find a Python list with the top ten FBI's Most Wanted names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ROBERT WILLIAM FISHER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ',\n",
       " 'EUGENE PALMER',\n",
       " 'RAFAEL CARO-QUINTERO',\n",
       " 'YASER ABDEL SAID']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "[i.text.strip('\\n') for i in soup.select('h3.title')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the date and time columns\n",
    "soup = BeautifulSoup(requests.get(url).content, 'lxml')\n",
    "date_time = [i.text.replace('\\xa0\\xa0\\xa0', ' ').split(' ') for i in soup.select('b a')]\n",
    "dateframe = pd.DataFrame(date_time, columns=['Date', 'Time']).iloc[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geting the rest of the columns of interest\n",
    "table = pd.read_html(url)[3]\n",
    "cols = [0,1,2,3,8,9,10,12]\n",
    "df = table.drop(table.columns[cols], axis=1)\n",
    "df.columns = ['Latitude','Degrees', 'Longitude', 'Degrees', 'Region Name']\n",
    "df1 = df.iloc[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Degrees</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Degrees</th>\n",
       "      <th>Region Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>07:33:43.0</td>\n",
       "      <td>30.68</td>\n",
       "      <td>S</td>\n",
       "      <td>71.32</td>\n",
       "      <td>W</td>\n",
       "      <td>COQUIMBO, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>07:29:29.0</td>\n",
       "      <td>30.62</td>\n",
       "      <td>S</td>\n",
       "      <td>71.98</td>\n",
       "      <td>W</td>\n",
       "      <td>OFFSHORE COQUIMBO, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>07:16:26.0</td>\n",
       "      <td>12.09</td>\n",
       "      <td>N</td>\n",
       "      <td>87.22</td>\n",
       "      <td>W</td>\n",
       "      <td>NEAR COAST OF NICARAGUA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>07:15:52.7</td>\n",
       "      <td>31.53</td>\n",
       "      <td>S</td>\n",
       "      <td>179.23</td>\n",
       "      <td>E</td>\n",
       "      <td>KERMADEC ISLANDS REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>07:10:53.0</td>\n",
       "      <td>17.09</td>\n",
       "      <td>N</td>\n",
       "      <td>94.86</td>\n",
       "      <td>W</td>\n",
       "      <td>OAXACA, MEXICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>07:06:15.8</td>\n",
       "      <td>26.83</td>\n",
       "      <td>N</td>\n",
       "      <td>55.27</td>\n",
       "      <td>E</td>\n",
       "      <td>SOUTHERN IRAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>07:05:34.2</td>\n",
       "      <td>19.17</td>\n",
       "      <td>N</td>\n",
       "      <td>155.47</td>\n",
       "      <td>W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>07:04:11.2</td>\n",
       "      <td>17.95</td>\n",
       "      <td>N</td>\n",
       "      <td>66.96</td>\n",
       "      <td>W</td>\n",
       "      <td>PUERTO RICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>07:01:30.0</td>\n",
       "      <td>42.00</td>\n",
       "      <td>N</td>\n",
       "      <td>142.50</td>\n",
       "      <td>E</td>\n",
       "      <td>HOKKAIDO, JAPAN REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>06:56:21.0</td>\n",
       "      <td>8.54</td>\n",
       "      <td>N</td>\n",
       "      <td>83.27</td>\n",
       "      <td>W</td>\n",
       "      <td>COSTA RICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>06:52:25.7</td>\n",
       "      <td>18.78</td>\n",
       "      <td>N</td>\n",
       "      <td>67.06</td>\n",
       "      <td>W</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>06:37:31.2</td>\n",
       "      <td>18.90</td>\n",
       "      <td>S</td>\n",
       "      <td>168.02</td>\n",
       "      <td>E</td>\n",
       "      <td>VANUATU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>06:31:25.0</td>\n",
       "      <td>13.42</td>\n",
       "      <td>N</td>\n",
       "      <td>120.11</td>\n",
       "      <td>E</td>\n",
       "      <td>MINDORO, PHILIPPINES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>06:17:11.2</td>\n",
       "      <td>38.02</td>\n",
       "      <td>N</td>\n",
       "      <td>27.41</td>\n",
       "      <td>E</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>06:16:26.0</td>\n",
       "      <td>9.12</td>\n",
       "      <td>N</td>\n",
       "      <td>82.73</td>\n",
       "      <td>W</td>\n",
       "      <td>PANAMA-COSTA RICA BORDER REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>06:09:41.5</td>\n",
       "      <td>18.33</td>\n",
       "      <td>N</td>\n",
       "      <td>68.00</td>\n",
       "      <td>W</td>\n",
       "      <td>MONA PASSAGE, DOMINICAN REPUBLIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>06:02:43.3</td>\n",
       "      <td>37.78</td>\n",
       "      <td>N</td>\n",
       "      <td>141.55</td>\n",
       "      <td>E</td>\n",
       "      <td>NEAR EAST COAST OF HONSHU, JAPAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>05:58:55.0</td>\n",
       "      <td>13.39</td>\n",
       "      <td>N</td>\n",
       "      <td>120.16</td>\n",
       "      <td>E</td>\n",
       "      <td>MINDORO, PHILIPPINES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>05:55:45.0</td>\n",
       "      <td>10.66</td>\n",
       "      <td>S</td>\n",
       "      <td>123.87</td>\n",
       "      <td>E</td>\n",
       "      <td>TIMOR REGION, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>05:45:54.9</td>\n",
       "      <td>19.06</td>\n",
       "      <td>N</td>\n",
       "      <td>155.42</td>\n",
       "      <td>W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>05:44:08.7</td>\n",
       "      <td>19.08</td>\n",
       "      <td>N</td>\n",
       "      <td>155.42</td>\n",
       "      <td>W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>05:35:59.2</td>\n",
       "      <td>17.98</td>\n",
       "      <td>N</td>\n",
       "      <td>67.09</td>\n",
       "      <td>W</td>\n",
       "      <td>PUERTO RICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>05:31:46.0</td>\n",
       "      <td>8.27</td>\n",
       "      <td>S</td>\n",
       "      <td>118.34</td>\n",
       "      <td>E</td>\n",
       "      <td>SUMBAWA REGION, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>05:28:04.0</td>\n",
       "      <td>31.31</td>\n",
       "      <td>S</td>\n",
       "      <td>71.50</td>\n",
       "      <td>W</td>\n",
       "      <td>COQUIMBO, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>05:26:01.6</td>\n",
       "      <td>19.12</td>\n",
       "      <td>N</td>\n",
       "      <td>66.44</td>\n",
       "      <td>W</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>05:19:51.0</td>\n",
       "      <td>8.32</td>\n",
       "      <td>N</td>\n",
       "      <td>82.91</td>\n",
       "      <td>W</td>\n",
       "      <td>PANAMA-COSTA RICA BORDER REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>04:56:52.0</td>\n",
       "      <td>37.87</td>\n",
       "      <td>S</td>\n",
       "      <td>71.11</td>\n",
       "      <td>W</td>\n",
       "      <td>ARAUCANIA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>04:34:38.0</td>\n",
       "      <td>8.12</td>\n",
       "      <td>S</td>\n",
       "      <td>107.90</td>\n",
       "      <td>E</td>\n",
       "      <td>JAVA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>04:15:23.5</td>\n",
       "      <td>35.35</td>\n",
       "      <td>N</td>\n",
       "      <td>3.63</td>\n",
       "      <td>W</td>\n",
       "      <td>STRAIT OF GIBRALTAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>04:10:04.2</td>\n",
       "      <td>45.69</td>\n",
       "      <td>N</td>\n",
       "      <td>7.05</td>\n",
       "      <td>E</td>\n",
       "      <td>NORTHERN ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>04:09:31.0</td>\n",
       "      <td>36.65</td>\n",
       "      <td>N</td>\n",
       "      <td>26.41</td>\n",
       "      <td>E</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>04:01:30.0</td>\n",
       "      <td>8.60</td>\n",
       "      <td>N</td>\n",
       "      <td>83.28</td>\n",
       "      <td>W</td>\n",
       "      <td>COSTA RICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>04:01:22.1</td>\n",
       "      <td>19.39</td>\n",
       "      <td>N</td>\n",
       "      <td>155.48</td>\n",
       "      <td>W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>03:52:21.0</td>\n",
       "      <td>8.04</td>\n",
       "      <td>S</td>\n",
       "      <td>114.72</td>\n",
       "      <td>E</td>\n",
       "      <td>BALI REGION, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>03:49:47.0</td>\n",
       "      <td>38.07</td>\n",
       "      <td>N</td>\n",
       "      <td>26.43</td>\n",
       "      <td>W</td>\n",
       "      <td>AZORES ISLANDS, PORTUGAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>03:43:25.0</td>\n",
       "      <td>21.42</td>\n",
       "      <td>S</td>\n",
       "      <td>68.50</td>\n",
       "      <td>W</td>\n",
       "      <td>ANTOFAGASTA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>03:35:18.1</td>\n",
       "      <td>19.13</td>\n",
       "      <td>N</td>\n",
       "      <td>155.50</td>\n",
       "      <td>W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>03:20:15.0</td>\n",
       "      <td>30.86</td>\n",
       "      <td>N</td>\n",
       "      <td>51.57</td>\n",
       "      <td>E</td>\n",
       "      <td>SOUTHERN IRAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>03:16:09.0</td>\n",
       "      <td>12.99</td>\n",
       "      <td>N</td>\n",
       "      <td>87.98</td>\n",
       "      <td>W</td>\n",
       "      <td>OFFSHORE EL SALVADOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>03:13:52.3</td>\n",
       "      <td>46.85</td>\n",
       "      <td>N</td>\n",
       "      <td>121.76</td>\n",
       "      <td>W</td>\n",
       "      <td>MOUNT RAINIER AREA, WASHINGTON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>03:04:32.1</td>\n",
       "      <td>38.19</td>\n",
       "      <td>N</td>\n",
       "      <td>23.46</td>\n",
       "      <td>E</td>\n",
       "      <td>GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>03:03:41.4</td>\n",
       "      <td>44.86</td>\n",
       "      <td>N</td>\n",
       "      <td>9.03</td>\n",
       "      <td>E</td>\n",
       "      <td>NORTHERN ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>03:00:32.0</td>\n",
       "      <td>23.98</td>\n",
       "      <td>S</td>\n",
       "      <td>67.13</td>\n",
       "      <td>W</td>\n",
       "      <td>SALTA, ARGENTINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>02:52:30.0</td>\n",
       "      <td>37.70</td>\n",
       "      <td>N</td>\n",
       "      <td>141.70</td>\n",
       "      <td>E</td>\n",
       "      <td>NEAR EAST COAST OF HONSHU, JAPAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>02:48:57.0</td>\n",
       "      <td>17.49</td>\n",
       "      <td>N</td>\n",
       "      <td>94.81</td>\n",
       "      <td>W</td>\n",
       "      <td>VERACRUZ, MEXICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>02:33:22.9</td>\n",
       "      <td>43.11</td>\n",
       "      <td>N</td>\n",
       "      <td>0.47</td>\n",
       "      <td>W</td>\n",
       "      <td>PYRENEES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>02:21:27.3</td>\n",
       "      <td>30.22</td>\n",
       "      <td>N</td>\n",
       "      <td>9.73</td>\n",
       "      <td>W</td>\n",
       "      <td>MOROCCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>02:20:35.1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>N</td>\n",
       "      <td>66.94</td>\n",
       "      <td>W</td>\n",
       "      <td>PUERTO RICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>02:14:41.6</td>\n",
       "      <td>37.21</td>\n",
       "      <td>N</td>\n",
       "      <td>3.67</td>\n",
       "      <td>W</td>\n",
       "      <td>SPAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>01:46:58.0</td>\n",
       "      <td>39.37</td>\n",
       "      <td>N</td>\n",
       "      <td>0.24</td>\n",
       "      <td>W</td>\n",
       "      <td>SPAIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date        Time Latitude Degrees Longitude Degrees  \\\n",
       "0   2021-02-18  07:33:43.0    30.68       S     71.32       W   \n",
       "1   2021-02-18  07:29:29.0    30.62       S     71.98       W   \n",
       "2   2021-02-18  07:16:26.0    12.09       N     87.22       W   \n",
       "3   2021-02-18  07:15:52.7    31.53       S    179.23       E   \n",
       "4   2021-02-18  07:10:53.0    17.09       N     94.86       W   \n",
       "5   2021-02-18  07:06:15.8    26.83       N     55.27       E   \n",
       "6   2021-02-18  07:05:34.2    19.17       N    155.47       W   \n",
       "7   2021-02-18  07:04:11.2    17.95       N     66.96       W   \n",
       "8   2021-02-18  07:01:30.0    42.00       N    142.50       E   \n",
       "9   2021-02-18  06:56:21.0     8.54       N     83.27       W   \n",
       "10  2021-02-18  06:52:25.7    18.78       N     67.06       W   \n",
       "11  2021-02-18  06:37:31.2    18.90       S    168.02       E   \n",
       "12  2021-02-18  06:31:25.0    13.42       N    120.11       E   \n",
       "13  2021-02-18  06:17:11.2    38.02       N     27.41       E   \n",
       "14  2021-02-18  06:16:26.0     9.12       N     82.73       W   \n",
       "15  2021-02-18  06:09:41.5    18.33       N     68.00       W   \n",
       "16  2021-02-18  06:02:43.3    37.78       N    141.55       E   \n",
       "17  2021-02-18  05:58:55.0    13.39       N    120.16       E   \n",
       "18  2021-02-18  05:55:45.0    10.66       S    123.87       E   \n",
       "19  2021-02-18  05:45:54.9    19.06       N    155.42       W   \n",
       "20  2021-02-18  05:44:08.7    19.08       N    155.42       W   \n",
       "21  2021-02-18  05:35:59.2    17.98       N     67.09       W   \n",
       "22  2021-02-18  05:31:46.0     8.27       S    118.34       E   \n",
       "23  2021-02-18  05:28:04.0    31.31       S     71.50       W   \n",
       "24  2021-02-18  05:26:01.6    19.12       N     66.44       W   \n",
       "25  2021-02-18  05:19:51.0     8.32       N     82.91       W   \n",
       "26  2021-02-18  04:56:52.0    37.87       S     71.11       W   \n",
       "27  2021-02-18  04:34:38.0     8.12       S    107.90       E   \n",
       "28  2021-02-18  04:15:23.5    35.35       N      3.63       W   \n",
       "29  2021-02-18  04:10:04.2    45.69       N      7.05       E   \n",
       "30  2021-02-18  04:09:31.0    36.65       N     26.41       E   \n",
       "31  2021-02-18  04:01:30.0     8.60       N     83.28       W   \n",
       "32  2021-02-18  04:01:22.1    19.39       N    155.48       W   \n",
       "33  2021-02-18  03:52:21.0     8.04       S    114.72       E   \n",
       "34  2021-02-18  03:49:47.0    38.07       N     26.43       W   \n",
       "35  2021-02-18  03:43:25.0    21.42       S     68.50       W   \n",
       "36  2021-02-18  03:35:18.1    19.13       N    155.50       W   \n",
       "37  2021-02-18  03:20:15.0    30.86       N     51.57       E   \n",
       "38  2021-02-18  03:16:09.0    12.99       N     87.98       W   \n",
       "39  2021-02-18  03:13:52.3    46.85       N    121.76       W   \n",
       "40  2021-02-18  03:04:32.1    38.19       N     23.46       E   \n",
       "41  2021-02-18  03:03:41.4    44.86       N      9.03       E   \n",
       "42  2021-02-18  03:00:32.0    23.98       S     67.13       W   \n",
       "43  2021-02-18  02:52:30.0    37.70       N    141.70       E   \n",
       "44  2021-02-18  02:48:57.0    17.49       N     94.81       W   \n",
       "45  2021-02-18  02:33:22.9    43.11       N      0.47       W   \n",
       "46  2021-02-18  02:21:27.3    30.22       N      9.73       W   \n",
       "47  2021-02-18  02:20:35.1    17.99       N     66.94       W   \n",
       "48  2021-02-18  02:14:41.6    37.21       N      3.67       W   \n",
       "49  2021-02-18  01:46:58.0    39.37       N      0.24       W   \n",
       "\n",
       "                         Region Name  \n",
       "0                    COQUIMBO, CHILE  \n",
       "1           OFFSHORE COQUIMBO, CHILE  \n",
       "2            NEAR COAST OF NICARAGUA  \n",
       "3            KERMADEC ISLANDS REGION  \n",
       "4                     OAXACA, MEXICO  \n",
       "5                      SOUTHERN IRAN  \n",
       "6           ISLAND OF HAWAII, HAWAII  \n",
       "7                        PUERTO RICO  \n",
       "8             HOKKAIDO, JAPAN REGION  \n",
       "9                         COSTA RICA  \n",
       "10                PUERTO RICO REGION  \n",
       "11                           VANUATU  \n",
       "12              MINDORO, PHILIPPINES  \n",
       "13                    WESTERN TURKEY  \n",
       "14   PANAMA-COSTA RICA BORDER REGION  \n",
       "15  MONA PASSAGE, DOMINICAN REPUBLIC  \n",
       "16  NEAR EAST COAST OF HONSHU, JAPAN  \n",
       "17              MINDORO, PHILIPPINES  \n",
       "18           TIMOR REGION, INDONESIA  \n",
       "19          ISLAND OF HAWAII, HAWAII  \n",
       "20          ISLAND OF HAWAII, HAWAII  \n",
       "21                       PUERTO RICO  \n",
       "22         SUMBAWA REGION, INDONESIA  \n",
       "23                   COQUIMBO, CHILE  \n",
       "24                PUERTO RICO REGION  \n",
       "25   PANAMA-COSTA RICA BORDER REGION  \n",
       "26                  ARAUCANIA, CHILE  \n",
       "27                   JAVA, INDONESIA  \n",
       "28               STRAIT OF GIBRALTAR  \n",
       "29                    NORTHERN ITALY  \n",
       "30        DODECANESE ISLANDS, GREECE  \n",
       "31                        COSTA RICA  \n",
       "32          ISLAND OF HAWAII, HAWAII  \n",
       "33            BALI REGION, INDONESIA  \n",
       "34          AZORES ISLANDS, PORTUGAL  \n",
       "35                ANTOFAGASTA, CHILE  \n",
       "36          ISLAND OF HAWAII, HAWAII  \n",
       "37                     SOUTHERN IRAN  \n",
       "38              OFFSHORE EL SALVADOR  \n",
       "39    MOUNT RAINIER AREA, WASHINGTON  \n",
       "40                            GREECE  \n",
       "41                    NORTHERN ITALY  \n",
       "42                  SALTA, ARGENTINA  \n",
       "43  NEAR EAST COAST OF HONSHU, JAPAN  \n",
       "44                  VERACRUZ, MEXICO  \n",
       "45                          PYRENEES  \n",
       "46                           MOROCCO  \n",
       "47                       PUERTO RICO  \n",
       "48                             SPAIN  \n",
       "49                             SPAIN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combining both frames to a single dataframe\n",
    "latest_20_earthquakes = pd.concat([dateframe, df1], axis =1)\n",
    "latest_20_earthquakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of tweets by a given Twitter account.\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['English', '6', '245', '000+', 'articles'],\n",
       " ['日本語', '1', '252', '000+', '記事'],\n",
       " ['Deutsch', '2', '534', '000+', 'Artikel'],\n",
       " ['Español', '1', '659', '000+', 'artículos'],\n",
       " ['Русский', '1', '697', '000+', 'статей'],\n",
       " ['Français', '2', '296', '000+', 'articles'],\n",
       " ['Italiano', '1', '672', '000+', 'voci'],\n",
       " ['中文', '1', '175', '000+', '條目'],\n",
       " ['Polski', '1', '454', '000+', 'haseł'],\n",
       " ['Português', '1', '055', '000+', 'artigos']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(requests.get(url).content)\n",
    "[i.text.strip('\\n\\n').split() for i in soup.select('div.central-featured-lang')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(requests.get(url).content)\n",
    "[i.text for i in soup.select('h3>a')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the top 10 languages by number of native speakers stored in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Language</th>\n",
       "      <th>Nativespeakersin millions2007 (2010)</th>\n",
       "      <th>Percentageof worldpopulation(2007)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mandarin (entire branch)</td>\n",
       "      <td>935 (955)</td>\n",
       "      <td>14.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>390 (405)</td>\n",
       "      <td>5.85%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>English</td>\n",
       "      <td>365 (360)</td>\n",
       "      <td>5.52%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hindi[a]</td>\n",
       "      <td>295 (310)</td>\n",
       "      <td>4.46%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>280 (295)</td>\n",
       "      <td>4.23%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>205 (215)</td>\n",
       "      <td>3.08%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Bengali</td>\n",
       "      <td>200 (205)</td>\n",
       "      <td>3.05%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Russian</td>\n",
       "      <td>160 (155)</td>\n",
       "      <td>2.42%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>125 (125)</td>\n",
       "      <td>1.92%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Punjabi</td>\n",
       "      <td>95 (100)</td>\n",
       "      <td>1.44%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank                  Language Nativespeakersin millions2007 (2010)  \\\n",
       "0    1  Mandarin (entire branch)                            935 (955)   \n",
       "1    2                   Spanish                            390 (405)   \n",
       "2    3                   English                            365 (360)   \n",
       "3    4                  Hindi[a]                            295 (310)   \n",
       "4    5                    Arabic                            280 (295)   \n",
       "5    6                Portuguese                            205 (215)   \n",
       "6    7                   Bengali                            200 (205)   \n",
       "7    8                   Russian                            160 (155)   \n",
       "8    9                  Japanese                            125 (125)   \n",
       "9   10                   Punjabi                             95 (100)   \n",
       "\n",
       "  Percentageof worldpopulation(2007)  \n",
       "0                              14.1%  \n",
       "1                              5.85%  \n",
       "2                              5.52%  \n",
       "3                              4.46%  \n",
       "4                              4.23%  \n",
       "5                              3.08%  \n",
       "6                              3.05%  \n",
       "7                              2.42%  \n",
       "8                              1.92%  \n",
       "9                              1.44%  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_html(url)[3]\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display IMDB's top 250 data (movie name, initial release, director name and stars) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie_Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Director</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Les Évadés</td>\n",
       "      <td>1994</td>\n",
       "      <td>Frank Darabont (dir.)</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Le parrain</td>\n",
       "      <td>1972</td>\n",
       "      <td>Francis Ford Coppola (dir.)</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Le parrain, 2ème partie</td>\n",
       "      <td>1974</td>\n",
       "      <td>Francis Ford Coppola (dir.)</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight : Le Chevalier noir</td>\n",
       "      <td>2008</td>\n",
       "      <td>Christopher Nolan (dir.)</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 hommes en colère</td>\n",
       "      <td>1957</td>\n",
       "      <td>Sidney Lumet (dir.)</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Drishyam</td>\n",
       "      <td>2015</td>\n",
       "      <td>Nishikant Kamat (dir.)</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>La bataille d'Alger</td>\n",
       "      <td>1966</td>\n",
       "      <td>Gillo Pontecorvo (dir.)</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Neon Genesis Evangelion: The End of Evangelion</td>\n",
       "      <td>1997</td>\n",
       "      <td>Hideaki Anno (dir.)</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Trois couleurs: Rouge</td>\n",
       "      <td>1994</td>\n",
       "      <td>Krzysztof Kieslowski (dir.)</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>7. Kogustaki Mucize</td>\n",
       "      <td>2019</td>\n",
       "      <td>Mehmet Ada Öztekin (dir.)</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Movie_Name  Year  \\\n",
       "0                                        Les Évadés  1994   \n",
       "1                                        Le parrain  1972   \n",
       "2                           Le parrain, 2ème partie  1974   \n",
       "3               The Dark Knight : Le Chevalier noir  2008   \n",
       "4                               12 hommes en colère  1957   \n",
       "..                                              ...   ...   \n",
       "245                                        Drishyam  2015   \n",
       "246                             La bataille d'Alger  1966   \n",
       "247  Neon Genesis Evangelion: The End of Evangelion  1997   \n",
       "248                           Trois couleurs: Rouge  1994   \n",
       "249                             7. Kogustaki Mucize  2019   \n",
       "\n",
       "                        Director Rating  \n",
       "0          Frank Darabont (dir.)    9.2  \n",
       "1    Francis Ford Coppola (dir.)    9.1  \n",
       "2    Francis Ford Coppola (dir.)    9.0  \n",
       "3       Christopher Nolan (dir.)    9.0  \n",
       "4            Sidney Lumet (dir.)    8.9  \n",
       "..                           ...    ...  \n",
       "245       Nishikant Kamat (dir.)    8.0  \n",
       "246      Gillo Pontecorvo (dir.)    8.0  \n",
       "247          Hideaki Anno (dir.)    8.0  \n",
       "248  Krzysztof Kieslowski (dir.)    8.0  \n",
       "249    Mehmet Ada Öztekin (dir.)    8.0  \n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(requests.get(url).content)\n",
    "\n",
    "#list of year released\n",
    "year_released = [i.text.strip('()') for i in soup.select('span.secondaryInfo')]\n",
    "\n",
    "#list of movies\n",
    "x = [i.text.strip(' \\n') for i in soup.select('td>a')]\n",
    "movie_name = [i for i in x if i != '']\n",
    "\n",
    "#list of directors\n",
    "y = [i['title'].split(',')[0] for i in soup.select('a[title]')]\n",
    "director = [i for i in y if '(dir.)' in i]\n",
    "\n",
    "#list of stars\n",
    "stars = [i.text for i in soup.select('strong')]\n",
    "\n",
    "#generating a dataframe\n",
    "df = pd.DataFrame(list(zip(movie_name, year_released, director, stars)), columns=['Movie_Name', 'Year', 'Director', 'Rating'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(year_released))\n",
    "# print(len(movie_name))\n",
    "# print(len(director))\n",
    "# print(len(stars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the city: paris\n"
     ]
    }
   ],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = input('Enter the city: ')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the book name, price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(requests.get(url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the ...</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History ...</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Requiem Red</td>\n",
       "      <td>£22.65</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Dirty Little Secrets ...</td>\n",
       "      <td>£33.34</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Coming Woman: A ...</td>\n",
       "      <td>£17.93</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Boys in the ...</td>\n",
       "      <td>£22.60</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Black Maria</td>\n",
       "      <td>£52.15</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Starving Hearts (Triangular Trade ...</td>\n",
       "      <td>£13.99</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shakespeare's Sonnets</td>\n",
       "      <td>£20.66</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Set Me Free</td>\n",
       "      <td>£17.46</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Scott Pilgrim's Precious Little ...</td>\n",
       "      <td>£52.29</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rip it Up and ...</td>\n",
       "      <td>£35.02</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Our Band Could Be ...</td>\n",
       "      <td>£57.25</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Olio</td>\n",
       "      <td>£23.88</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mesaerion: The Best Science ...</td>\n",
       "      <td>£37.59</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Libertarianism for Beginners</td>\n",
       "      <td>£51.33</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>It's Only the Himalayas</td>\n",
       "      <td>£45.17</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Name   Price     Stock\n",
       "0                      A Light in the ...  £51.77  In stock\n",
       "1                      Tipping the Velvet  £53.74  In stock\n",
       "2                              Soumission  £50.10  In stock\n",
       "3                           Sharp Objects  £47.82  In stock\n",
       "4            Sapiens: A Brief History ...  £54.23  In stock\n",
       "5                         The Requiem Red  £22.65  In stock\n",
       "6            The Dirty Little Secrets ...  £33.34  In stock\n",
       "7                 The Coming Woman: A ...  £17.93  In stock\n",
       "8                     The Boys in the ...  £22.60  In stock\n",
       "9                         The Black Maria  £52.15  In stock\n",
       "10  Starving Hearts (Triangular Trade ...  £13.99  In stock\n",
       "11                  Shakespeare's Sonnets  £20.66  In stock\n",
       "12                            Set Me Free  £17.46  In stock\n",
       "13    Scott Pilgrim's Precious Little ...  £52.29  In stock\n",
       "14                      Rip it Up and ...  £35.02  In stock\n",
       "15                  Our Band Could Be ...  £57.25  In stock\n",
       "16                                   Olio  £23.88  In stock\n",
       "17        Mesaerion: The Best Science ...  £37.59  In stock\n",
       "18           Libertarianism for Beginners  £51.33  In stock\n",
       "19                It's Only the Himalayas  £45.17  In stock"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = [i.text for i in soup.select('h3>a')]\n",
    "price = [i.text.strip() for i in soup.select('p.price_color')]\n",
    "stock = [i.text.strip() for i in soup.select('p.instock')]\n",
    "data =list(zip(name, price, stock))\n",
    "df = pd.DataFrame(data, columns=['Name', 'Price', 'Stock'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(name))\n",
    "# print(len(price))\n",
    "# print(len(stock))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
